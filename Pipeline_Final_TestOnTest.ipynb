{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process functions defined~\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "# to convert the json files of the Train, Dev, Test datasets to lists for fitting ML models\n",
    "# the same as in the Dev script\n",
    "Path_Dev_en = \"Train_Dev_sets\\\\Dev_en_new.json\"\n",
    "Path_Dev_fr = \"Train_Dev_sets\\\\Dev_fr_new.json\"\n",
    "\n",
    "Path_Train_en = \"Train_Dev_sets\\\\Train_en_new.json\"\n",
    "Path_Train_fr = \"Train_Dev_sets\\\\Train_fr_new.json\"\n",
    "\n",
    "Path_Test_en = \"Test_set\\\\Test_en.json\"\n",
    "Path_Test_fr = \"Test_set\\\\Test_fr.json\"\n",
    "\n",
    "def load_with_index(path): # for Train and Dev files\n",
    "    with open(path) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "\n",
    "    list_words =  data['text'].split(' ')\n",
    "    list_begins =  data['begin_sentence']\n",
    "    list_ends =  data['end_sentence']\n",
    "    \n",
    "    list_convert_words = []\n",
    "    for word in list_words:\n",
    "        if word.find('\\n')>=0:\n",
    "            list_convert_words.append(word.strip('\\n')+'_Enter') # _Enter is used to replace \\n \n",
    "        else:\n",
    "            list_convert_words.append(word)\n",
    "       \n",
    "    return list_convert_words, list_begins, list_ends\n",
    "\n",
    "def load_without_index(path): # for Test files\n",
    "    with open(path) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "\n",
    "    list_words =  data['text'].split(' ')\n",
    "    \n",
    "    list_convert_words = []\n",
    "    for word in list_words:\n",
    "        if word.find('\\n')>=0:\n",
    "            list_convert_words.append(word.strip('\\n')+'_Enter') \n",
    "        else:\n",
    "            list_convert_words.append(word)\n",
    "   \n",
    "    return list_convert_words\n",
    "    \n",
    "def labelled_text(convert_text, list_begins, list_ends):\n",
    "    labelled_list_words = []\n",
    "\n",
    "    for index, item in enumerate(convert_text):\n",
    "    \n",
    "        if index in list_begins:\n",
    "            labelled_list_words.append(item+'_BEGIN')\n",
    "        \n",
    "        elif index in list_ends:\n",
    "            labelled_list_words.append(item+'_END')\n",
    "        \n",
    "        else:\n",
    "            labelled_list_words.append(item)\n",
    "\n",
    "    return labelled_list_words\n",
    "\n",
    "def write_text(file, content):\n",
    "    outfile = open(file, 'w', encoding='utf-8')\n",
    "    for word in content:\n",
    "        outfile.write(word+'\\n')\n",
    "    outfile.close()\n",
    "\n",
    "print (\"Pre-process functions defined~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Train, Dev, Test files\n",
    "Dev_en_words, Dev_en_begins, Dev_en_ends = load_with_index(Path_Dev_en)\n",
    "Dev_fr_words, Dev_fr_begins, Dev_fr_ends = load_with_index(Path_Dev_fr)\n",
    "Train_en_words, Train_en_begins, Train_en_ends = load_with_index(Path_Train_en)\n",
    "Train_fr_words, Train_fr_begins, Train_fr_ends = load_with_index(Path_Train_fr)\n",
    "Test_en_words = load_without_index(Path_Test_en)\n",
    "Test_fr_words = load_without_index(Path_Test_fr)\n",
    "\n",
    "# generate labelled texts\n",
    "Dev_en_labelled = labelled_text(Dev_en_words, Dev_en_begins, Dev_en_ends)\n",
    "Dev_fr_labelled = labelled_text(Dev_fr_words, Dev_fr_begins, Dev_fr_ends)\n",
    "Train_en_labelled = labelled_text(Train_en_words, Train_en_begins, Train_en_ends)\n",
    "Train_fr_labelled = labelled_text(Train_fr_words, Train_fr_begins, Train_fr_ends)\n",
    "\n",
    "# write Train, Dev, Test text files\n",
    "write_text(\"Pre-processed Datasets\\\\Dev_en_text.txt\", Dev_en_words)\n",
    "write_text(\"Pre-processed Datasets\\\\Dev_en_text_labelled.txt\", Dev_en_labelled)\n",
    "\n",
    "write_text(\"Pre-processed Datasets\\\\Dev_fr_text.txt\", Dev_fr_words)\n",
    "write_text(\"Pre-processed Datasets\\\\Dev_fr_text_labelled.txt\", Dev_fr_labelled)\n",
    "\n",
    "write_text(\"Pre-processed Datasets\\\\Train_en_text.txt\", Train_en_words)\n",
    "write_text(\"Pre-processed Datasets\\\\Train_en_text_labelled.txt\", Train_en_labelled)\n",
    "\n",
    "write_text(\"Pre-processed Datasets\\\\Train_fr_text.txt\", Train_fr_words)\n",
    "write_text(\"Pre-processed Datasets\\\\Train_fr_text_labelled.txt\", Train_fr_labelled)\n",
    "\n",
    "write_text(\"Pre-processed Datasets\\\\Test_en_text.txt\", Test_en_words)\n",
    "write_text(\"Pre-processed Datasets\\\\Test_fr_text.txt\", Test_fr_words)\n",
    "\n",
    "print(\"Data pre-processing completed~\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Classifier Selection \n",
    "Classifier: Random Forest Classifier (sklearn rfc). we also tried mnb, lr, dtc, knc, svm, and rfc is the best for this task.\n",
    "##### The most optimized parameters: \n",
    "min_samples_split = 8, max_features = “log2”, oob_score = True, random_state = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rfc classifier defined with min_samples_split = 8, max_features = log2, oob_score = True, random_state = 10 ~\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rfc( X_train, Y_train, X_test):\n",
    "\n",
    "    model = RandomForestClassifier( min_samples_split = 8, max_features = \"log2\", oob_score = True, random_state = 10 )\n",
    "    nn = model.fit( X_train, Y_train )\n",
    "\n",
    "    pre_test = model.predict( X_test )\n",
    "\n",
    "    #target_names = [\"O\",\"BS\",\"ES\"] \n",
    "    #target_names = [\"BS\",\"ES\",\"O\"] \n",
    "    #print( classification_report( Y_test, pre_test, target_names = target_names, digits=5 ) )\n",
    "\n",
    "    return pre_test\n",
    "\n",
    "print(\"The rfc classifier defined with min_samples_split = 8, max_features = log2, oob_score = True, random_state = 10 ~\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feature Engineering\n",
    "We tried features of:\n",
    "1. wordcount2vectors: not good and slow;\n",
    "2. individual features of punctuations, initially captalized words, acronyms, _Enter(\\n), digits, letters (including Roman numbers) and pos tags;\n",
    "3. feature fusion all the features in 2;\n",
    "4. rule-based validation with keyword list which was extracted from the Train and Dev datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature sets defined.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from sklearn import preprocessing\n",
    "from numpy import array\n",
    "\n",
    "#ERROR_INSTANCE_FN = 'error_cases.txt'\n",
    "FOLDER_TEXT = 'Pre-processed Datasets'\n",
    "FOLDER_POS = 'POS Tagged Datasets'\n",
    "#SAMPLE_WEIGHT_BEGIN = 1\n",
    "#SAMPLE_WEIGHT_END = 1\n",
    "PUNC_SET1  = ['.']\n",
    "PUNC_SET2  = ['?', '!', ';', ',', '%', '-', '/', '\\\"', '\\\\', '\\'', ')', '(', '*', '', '>', '≥', '<', '≤', '•', '€', '$', '£', '``', '©', '℗', '®']\n",
    "LETTER_SET = list(string.ascii_letters)\n",
    "\n",
    "def load_filecontent(fn):\n",
    "    text = list( open( fn, \"r\", encoding = 'utf-8' ).readlines() )\n",
    "    text = [s.strip('\\n') for s in text]\n",
    "   \n",
    "    return text\n",
    "\n",
    "def load_both_train():\n",
    "    train_file_en = FOLDER_TEXT + \"/Train_en_text_labelled.txt\"\n",
    "    train_en = load_filecontent(train_file_en)\n",
    "    \n",
    "    train_file_fr = FOLDER_TEXT + \"/Train_fr_text_labelled.txt\"\n",
    "    train_fr = load_filecontent(train_file_fr)\n",
    "      \n",
    "    dev_file_en = FOLDER_TEXT + \"/Dev_en_text_labelled.txt\"\n",
    "    dev_en = load_filecontent(dev_file_en)\n",
    "    \n",
    "    dev_file_fr = FOLDER_TEXT + \"/Dev_fr_text_labelled.txt\"\n",
    "    dev_fr = load_filecontent(dev_file_fr)\n",
    "    \n",
    "    train_data = train_en + train_fr + dev_en + dev_fr\n",
    "    \n",
    "    test_file_en = FOLDER_TEXT + \"/Test_en_text.txt\"\n",
    "    test_en = load_filecontent(test_file_en)\n",
    "    \n",
    "    test_file_fr = FOLDER_TEXT + \"/Test_fr_text.txt\"\n",
    "    test_fr = load_filecontent(test_file_fr)\n",
    "    \n",
    "    return train_data, test_en, test_fr\n",
    "\n",
    "def load_pos():\n",
    "    train_file_en = FOLDER_POS + \"/Train_en_udpipe_pos.txt\"\n",
    "    train_en_pos = load_filecontent(train_file_en)\n",
    "    \n",
    "    train_file_fr = FOLDER_POS + \"/Train_fr_udpipe_pos.txt\"\n",
    "    train_fr_pos = load_filecontent(train_file_fr)\n",
    "    \n",
    "    dev_file_en = FOLDER_POS + \"/Dev_en_udpipe_pos.txt\"\n",
    "    dev_en_pos = load_filecontent(dev_file_en)\n",
    "    \n",
    "    dev_file_fr = FOLDER_POS + \"/Dev_fr_udpipe_pos.txt\"\n",
    "    dev_fr_pos = load_filecontent(dev_file_fr)\n",
    "    \n",
    "    train_pos = train_en_pos + train_fr_pos + dev_en_pos + dev_fr_pos\n",
    "    \n",
    "    test_file_en = FOLDER_POS + \"/Test_en_udpipe_pos.txt\"\n",
    "    test_en_pos = load_filecontent(test_file_en)\n",
    "    \n",
    "    test_file_fr = FOLDER_POS + \"/Test_fr_udpipe_pos.txt\"\n",
    "    test_fr_pos = load_filecontent(test_file_fr)\n",
    "    \n",
    "    return train_pos, test_en_pos, test_fr_pos\n",
    "\n",
    "def find_pos_alter( dataset, trainlen ):\n",
    "    INV = '18'\n",
    "\n",
    "    pos_feature = []\n",
    "\n",
    "    pos_vec = [0] * len( dataset )\n",
    "    pos_vec[0] = [INV, INV, dataset[0], dataset[1], dataset[2]]\n",
    "    pos_vec[1] = [INV, dataset[0], dataset[1], dataset[2], dataset[3]]\n",
    "    for i in range( 2, len( dataset ) - 2 ):\n",
    "        pos_vec[i] = [dataset[i - 2], dataset[i - 1], dataset[i], dataset[i + 1], dataset[i + 2]]\n",
    "    pos_vec[-2] = [dataset[-4], dataset[-3], dataset[-2], dataset[-1], INV]\n",
    "    pos_vec[-1] = [dataset[-3], dataset[-2], dataset[-1], INV, INV]\n",
    "\n",
    "    pos_vec_temp = []\n",
    "    for item in pos_vec:\n",
    "        pos_vec_temp.append( [int( x ) for x in item] )\n",
    "\n",
    "    one_hot = preprocessing.OneHotEncoder( sparse = False )\n",
    "    temp = one_hot.fit_transform( pos_vec_temp )\n",
    "\n",
    "    return np.array( temp[0:trainlen] ), np.array( temp[trainlen:] )\n",
    "\n",
    "def find_cap(dataset):\n",
    "    cap_feature = []  # 3-dim, 1st, cap for the word, 2nd, cap for prev, 3rd, cap for next\n",
    "    \n",
    "    cap_bool = []\n",
    "    for word in dataset:\n",
    "        if word[0].isupper():\n",
    "            cap_bool.append( 1 )\n",
    "        else:\n",
    "            cap_bool.append( 0 )    \n",
    "    \n",
    "    # first word\n",
    "    cap_feature.append( [0, cap_bool[0], cap_bool[1]] )\n",
    "    \n",
    "    for i in range(1, len(cap_bool) - 1):\n",
    "        cap_feature.append( [cap_bool[i - 1], cap_bool[i], cap_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    cap_feature.append( [cap_bool[-2], cap_bool[-1], 0] )\n",
    "\n",
    "    return np.array( cap_feature )\n",
    "\n",
    "def find_num(dataset):\n",
    "    num_feature = []  # 3-dim, \n",
    "    \n",
    "    num_bool = []\n",
    "    for word in dataset:\n",
    "        if word[0].isdigit():\n",
    "            num_bool.append( 1 )\n",
    "        else:\n",
    "            num_bool.append( 0 )    \n",
    "    \n",
    "    # first word\n",
    "    num_feature.append( [0, num_bool[0], num_bool[1]] )\n",
    "    \n",
    "    for i in range(1, len(num_bool) - 1):\n",
    "        num_feature.append( [num_bool[i - 1], num_bool[i], num_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    num_feature.append( [num_bool[-2], num_bool[-1], 0] )\n",
    "\n",
    "    return np.array( num_feature )\n",
    "\n",
    "def find_letter(dataset):\n",
    "    letter_feature = []  # 3-dim, \n",
    "    \n",
    "    letter_bool = []\n",
    "    for word in dataset:\n",
    "        if word in LETTER_SET:\n",
    "            letter_bool.append( 1 )\n",
    "        else:\n",
    "            letter_bool.append( 0 )    \n",
    "    \n",
    "    # first word\n",
    "    letter_feature.append( [0, letter_bool[0], letter_bool[1]] )\n",
    "    \n",
    "    for i in range(1, len(letter_bool) - 1):\n",
    "        letter_feature.append( [letter_bool[i - 1], letter_bool[i], letter_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    letter_feature.append( [letter_bool[-2], letter_bool[-1], 0] )\n",
    "\n",
    "    return np.array( letter_feature )\n",
    "\n",
    "def find_Acronyms(dataset):\n",
    "    abbs_feature = []\n",
    "    \n",
    "    abbs_bool = [0] * len( dataset )\n",
    "\n",
    "    for i, word in enumerate( dataset ):\n",
    "        abbs_bool[i] = 0\n",
    "        for letter in word:\n",
    "            if not letter.isupper():\n",
    "                abbs_bool[i] = 0\n",
    "                break\n",
    "            abbs_bool[i] = 1\n",
    "\n",
    "    # first word\n",
    "    abbs_feature.append( [0, abbs_bool[0], abbs_bool[1]] )\n",
    "\n",
    "    for i in range( 1, len( abbs_bool ) - 1 ):\n",
    "        abbs_feature.append( [abbs_bool[i - 1], abbs_bool[i], abbs_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    abbs_feature.append( [abbs_bool[-2], abbs_bool[-1], 0] )\n",
    "\n",
    "    return np.array( abbs_feature )\n",
    "\n",
    "def find_punc(dataset, PUNC_SET):\n",
    "    punc_feature = []  # match punc \n",
    "\n",
    "    # the rule is not very strict at the beginning, could introduce some noise\n",
    "    punc_bool = [0] * len( dataset )\n",
    "    for i in range( len( dataset ) ):\n",
    "        for punc in PUNC_SET:\n",
    "            if punc in dataset[i]:\n",
    "                punc_bool[i] = 1\n",
    "                break\n",
    "\n",
    "    # first word\n",
    "    punc_feature.append( [0, punc_bool[0], punc_bool[1]] )\n",
    "\n",
    "    for i in range( 1, len( punc_bool ) - 1 ):\n",
    "        punc_feature.append( [punc_bool[i - 1], punc_bool[i], punc_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    punc_feature.append( [punc_bool[-2], punc_bool[-1], 0] )\n",
    "\n",
    "    return np.array( punc_feature )\n",
    "\n",
    "def find_ent(dataset):\n",
    "    ent_feature = []  # 3 dim, 1 for presence, 0 for non-presence of _Enter(\\n)\n",
    "\n",
    "    ent_bool = []\n",
    "    for word in dataset:\n",
    "        if word.find('_Enter') >= -1:\n",
    "            ent_bool.append( 1 )\n",
    "        else:\n",
    "            ent_bool.append( 0 )    \n",
    "    \n",
    "    # first word\n",
    "    ent_feature.append( [0, ent_bool[0], ent_bool[1]] )\n",
    "    \n",
    "    for i in range(1, len(ent_bool) - 1):\n",
    "        ent_feature.append( [ent_bool[i - 1], ent_bool[i], ent_bool[i + 1]] )\n",
    "\n",
    "    # last word\n",
    "    ent_feature.append( [ent_bool[-2], ent_bool[-1], 0] )\n",
    "\n",
    "    return np.array( ent_feature )\n",
    "\n",
    "def construct_keyword_pattern_dict( dataset ):\n",
    "    pattern_dict = {}\n",
    "\n",
    "    for i, word in enumerate( dataset[0:-1] ):\n",
    "        j = 0\n",
    "        if '_END' in dataset[i]:\n",
    "            if '_BEGIN' in dataset[i + 1]:\n",
    "                continue\n",
    "            else:\n",
    "#                 new_pattern = dataset[i + 1]\n",
    "                new_pattern = ''\n",
    "                try:\n",
    "                    while '_BEGIN' not in dataset[i + j + 1]:\n",
    "                        try:\n",
    "                            new_pattern = new_pattern + '<sssss>' + dataset[i + j + 1]\n",
    "                        except:\n",
    "                            break\n",
    "                        j += 1\n",
    "\n",
    "                    new_pattern_cleaned = new_pattern[7:]\n",
    "                    if pattern_dict.__contains__( new_pattern_cleaned ):\n",
    "                        pattern_dict[new_pattern_cleaned] += 1\n",
    "                    else:\n",
    "                        pattern_dict[new_pattern_cleaned] = 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "    return pattern_dict\n",
    "\n",
    "def find_negbdry_pattern( train_data):\n",
    "    negbdry_set_train = construct_keyword_pattern_dict( train_data )\n",
    "\n",
    "    return negbdry_set_train\n",
    "\n",
    "LEN_LIMIT = 5\n",
    "FREQ_LIMIT = 5\n",
    "\n",
    "def rule_based_validation(pattern_dict, dataset, pre_test):\n",
    "    temp_test = copy.deepcopy( pre_test )\n",
    "\n",
    "    NON_BOUNDARY_MARK = \"O\"\n",
    "    \n",
    "    for pattern_str in pattern_dict:\n",
    "        pattern_series = pattern_str.split('<sssss>')\n",
    "        pattern_len = len(pattern_series)\n",
    "        \n",
    "        # filter\n",
    "        if pattern_len < LEN_LIMIT:\n",
    "            continue\n",
    "\n",
    "        if pattern_dict[pattern_str] < FREQ_LIMIT:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        for i in range(len(dataset) - pattern_len):\n",
    "            b_matched = False\n",
    "            for j in range(pattern_len):\n",
    "                if dataset[i + j] != pattern_series[j]:\n",
    "                    break\n",
    "                b_matched = True\n",
    "                \n",
    "            if b_matched:\n",
    "                for j in range( pattern_len ):\n",
    "                    # correct prediction\n",
    "                    temp_test[i + j] = NON_BOUNDARY_MARK\n",
    "\n",
    "    return temp_test\n",
    "\n",
    "def dict_refinement(dict_keyword):\n",
    "    dict_refined = {}\n",
    "    \n",
    "    for item in dict_keyword.keys():\n",
    "        newitem = item\n",
    "#         newitem = item.replace('<sssss>',' ')\n",
    "        if '_Enter' in newitem:\n",
    "            keywords = newitem.split('_Enter')\n",
    "            \n",
    "            keyword_refined = keywords[0] + '_Enter'\n",
    "            if dict_refined.__contains__( keyword_refined ):\n",
    "                dict_refined[keyword_refined] += dict_keyword[item]\n",
    "            else:\n",
    "                dict_refined[keyword_refined] = dict_keyword[item]\n",
    "                \n",
    "            for keyword in keywords[1:]:\n",
    "                keyword_refined = keyword + '_Enter'\n",
    "                if dict_refined.__contains__( keyword_refined ):\n",
    "                    dict_refined[keyword_refined[7:]] += dict_keyword[item]\n",
    "                else:\n",
    "                    dict_refined[keyword_refined[7:]] = dict_keyword[item]\n",
    "        else:\n",
    "            if dict_refined.__contains__( item ):\n",
    "                dict_refined[newitem] += dict_keyword[item]\n",
    "            else:\n",
    "                dict_refined[newitem] = dict_keyword[item]\n",
    "            \n",
    "    fp = open('keywordlist.txt', 'w', encoding='utf8')\n",
    "    \n",
    "    sorted_list = sorted(dict_refined.items(), key=lambda item:item[1], reverse =True)\n",
    "    \n",
    "    for item, value in sorted_list:\n",
    "        newitem = item.replace('<sssss>',' ')\n",
    "        newitem = newitem.replace('_Enter','')\n",
    "        fp.writelines(newitem + '\\t' + str(value) + '\\n')\n",
    "    fp.close()\n",
    "            \n",
    "    return dict_refined\n",
    "\n",
    "def gen_label( dataset ):\n",
    "    labels = []\n",
    "\n",
    "    for line in dataset:\n",
    "        if '_BEGIN' in line:\n",
    "            labels.append( \"BS\" )  # 'BEGIN')\n",
    "        elif '_END' in line:\n",
    "            labels.append( \"ES\" )  # 'END')\n",
    "        else:\n",
    "            labels.append( \"O\" )  # '_')\n",
    "\n",
    "    return np.array( labels )\n",
    "\n",
    "def gen_sample_weight( dataset ):\n",
    "    sample_weight = []\n",
    "\n",
    "    for line in dataset:\n",
    "        if '_BEGIN' in line:\n",
    "            sample_weight.append( SAMPLE_WEIGHT_BEGIN )  # 'BEGIN')\n",
    "        elif '_END' in line:\n",
    "            sample_weight.append( SAMPLE_WEIGHT_END )  # 'END')\n",
    "        else:\n",
    "            sample_weight.append( 1 )  # '_')\n",
    "\n",
    "    return np.array( sample_weight )\n",
    "\n",
    "def error_print(Y_test, pre_test, test_data):\n",
    "    \n",
    "    fp = open( ERROR_INSTANCE_FN, 'w', encoding = 'utf8' )\n",
    "\n",
    "    for i in range( len( Y_test ) ):\n",
    "        if pre_test[i] == \"BS\":\n",
    "            pre_label = 'BEGIN'\n",
    "        elif pre_test[i] == \"ES\":\n",
    "            pre_label = 'END'\n",
    "        elif pre_test[i] == \"O\":\n",
    "            pre_label = 'NoBoun'\n",
    "\n",
    "        if Y_test[i] != pre_test[i]:\n",
    "            fp.writelines( test_data[i] + '____ERROR with ' + pre_label + '\\n' )\n",
    "        else:\n",
    "            fp.writelines( test_data[i] + '\\n' )\n",
    "\n",
    "def train_test( model):\n",
    "\n",
    "    print( 'using punc_set1, punc_set2, cap, acro, num, letter, enter, pos' )\n",
    "    pre_test = model( train_vec, train_lbl, test_vec )\n",
    "    #error_print(Y_test, pre_test, test_data)\n",
    "\n",
    "    return pre_test\n",
    "\n",
    "print(\"Feature sets defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ using all features $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$:\n",
      "using punc_set1, punc_set2, cap, acro, num, letter, enter, pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:463: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$ post processing with keyword validation $$$$$$$$$$$$$$$$$$$$:\n",
      "3 16\n",
      "Predictions completed.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_data, test_en, test_fr = load_both_train()\n",
    "    train_pos, test_en_pos, test_fr_pos = load_pos()\n",
    "    \n",
    "     #test on the English Test dataset\n",
    "    test_data = test_en\n",
    "    test_pos = test_en_pos\n",
    "    \n",
    "    #test on the French Test dataset\n",
    "    #test_data = test_fr\n",
    "    #test_pos = test_fr_pos\n",
    "    \n",
    "    train_pos_feature, test_pos_feature = find_pos_alter( train_pos + test_pos, len( train_pos ) )\n",
    "\n",
    "    train_cap = find_cap( train_data )\n",
    "    test_cap = find_cap( test_data )\n",
    "    \n",
    "    train_num = find_num( train_data )\n",
    "    test_num = find_num( test_data )\n",
    "\n",
    "    train_abbs = find_Acronyms( train_data )\n",
    "    test_abbs = find_Acronyms( test_data )\n",
    "\n",
    "    train_letter = find_letter( train_data )\n",
    "    test_letter = find_letter( test_data )\n",
    "\n",
    "    train_punc1 = find_punc( train_data, PUNC_SET1 )\n",
    "    test_punc1 = find_punc( test_data , PUNC_SET1 )\n",
    "    \n",
    "    train_punc2 = find_punc( train_data, PUNC_SET2 )\n",
    "    test_punc2 = find_punc( test_data , PUNC_SET2 )\n",
    "\n",
    "    train_ent = find_ent( train_data )\n",
    "    test_ent = find_ent( test_data )\n",
    "    \n",
    "    # feature fusion\n",
    "    #train_vec = np.concatenate( (train_punc1, train_punc2, train_num, train_cap, train_abbs, train_ent, train_letter, train_pos_feature ), axis = 1 )\n",
    "    #test_vec = np.concatenate( ( test_punc1, test_punc2, test_num, test_cap, test_abbs, test_ent, test_letter, test_pos_feature), axis = 1 )\n",
    "    train_vec = np.concatenate( (train_punc1, train_num, train_cap, train_abbs, train_ent, train_letter, train_pos_feature ), axis = 1 )\n",
    "    test_vec = np.concatenate( ( test_punc1, test_num, test_cap, test_abbs, test_ent, test_letter, test_pos_feature), axis = 1 )\n",
    "    # build label and weight vector\n",
    "    train_lbl = gen_label( train_data )\n",
    "    #test_lbl = gen_label( test_data )\n",
    "\n",
    "    #train_wgt = gen_sample_weight( train_data )\n",
    "    #test_wgt = gen_sample_weight( test_data )\n",
    "\n",
    "    print( '$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ using all features $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$:' )\n",
    "    model = rfc\n",
    "    pre_test = train_test( model)\n",
    "    \n",
    "\n",
    "    print( '$$$$$$$$$$$$$$$$$$$$ post processing with keyword validation $$$$$$$$$$$$$$$$$$$$:' )\n",
    "    negbdry_set_train = find_negbdry_pattern( train_data)\n",
    "\n",
    "    dict_refined = dict_refinement( negbdry_set_train )\n",
    "\n",
    "    LEN_LIMIT = 3\n",
    "    FREQ_LIMIT = 16\n",
    "    print(  LEN_LIMIT, FREQ_LIMIT )\n",
    "    pre_test_valid = rule_based_validation( dict_refined, test_data, pre_test )\n",
    "    \n",
    "    print(  \"Predictions completed.\" )\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write predictions to files (text file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction files are written~\n"
     ]
    }
   ],
   "source": [
    " # to write the prediction to a file\n",
    "write_text(\"Output/Predict_Test_en_valid.txt\", pre_test_valid)\n",
    "write_text(\"Output/Predict_Test_en_novalid.txt\", pre_test.tolist())\n",
    "#write_text(\"Output/Predict_Test_fr_valid.txt\", pre_test_valid)\n",
    "#write_text(\"Output/Predict_Test_fr_novalid.txt\", pre_test.tolist())\n",
    "\n",
    "\n",
    "print(\"Prediction files are written~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write predictions to a json file (for the organizer to validate the classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test json files written~ Ready for validation\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def index_generation(list_label):\n",
    "    index_begin = []\n",
    "    index_end = []\n",
    "    \n",
    "    index = 0\n",
    "    for label in list_label:\n",
    "        if label == \"BS\":\n",
    "            index_begin.append(index)\n",
    "        elif label == \"ES\":\n",
    "            index_end.append(index)\n",
    "        index += 1\n",
    "            \n",
    "    return index_begin, index_end\n",
    "\n",
    "Index_begins, Index_ends = index_generation(pre_test)\n",
    "Index_begins_valid, Index_ends_valid = index_generation(pre_test_valid)\n",
    "\n",
    "mydict = {}\n",
    "mydict_valid = {}\n",
    "\n",
    "Test_en_words = load_without_index(Path_Test_en)\n",
    "Test_en_words = [word.replace(\"_Enter\", \"\\n\") for word in Test_en_words]\n",
    "#Test_fr_words = load_without_index(Path_Test_fr)\n",
    "#Test_fr_words = [word.replace(\"_Enter\", \"\\n\") for word in Test_fr_words]\n",
    "\n",
    "text = ' '.join(Test_en_words)\n",
    "#text = ' '.join(Test_fr_words)\n",
    "mydict[\"text\"] = text\n",
    "mydict[\"begin_sentence\"] = Index_begins\n",
    "mydict[\"end_sentence\"] = Index_ends\n",
    "\n",
    "mydict_valid[\"text\"] = text\n",
    "mydict_valid[\"begin_sentence\"] = Index_begins_valid\n",
    "mydict_valid[\"end_sentence\"] = Index_ends_valid\n",
    "\n",
    "with open('Test_Predicted/Test_en_pred_valid.json', 'w') as fp:\n",
    "    json.dump(mydict, fp)\n",
    "with open('Test_Predicted/Test_en_pred_novalid.json', 'w') as fp:\n",
    "    json.dump(mydict_valid, fp)\n",
    "    \n",
    "#with open('Test_Predicted/Test_fr_pred_valid.json', 'w') as fp:\n",
    "    #json.dump(mydict, fp)\n",
    "#with open('Test_Predicted/Test_fr_pred_novalid.json', 'w') as fp:\n",
    "    #json.dump(mydict_valid, fp)\n",
    "    \n",
    "print(\"Test json files written~ Ready for validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
